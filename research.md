My research program is dedicated to bridging the gap between the theoretical elegance of pure mathematics and the practical power of machine learning. I am passionate about using foundational principles from abstract algebra to build the next generation of artificial intelligence models. My core focus is on developing learning frameworks that produce more structured, robust, and data-efficient representations, with primary applications in computer vision and natural language processing.

The central idea of my work is to design models that learn to create an internal "map" of the data, where the relationships and transformations in the learned feature space accurately mirror the meaningful relationships in the original data. By enforcing this structural consistency, I aim to move beyond simple pattern recognition and toward models that possess a deeper, more semantic understanding. This approach directly addresses critical challenges in AI, including model interpretability, the need for massive labeled datasets, and the logical consistency of model outputs.

Research Interests
My research interests are unified by this theme of structure-driven learning. I apply this foundational philosophy across several key domains in modern AI:

Self-Supervised Learning (SSL): This is the primary vehicle for my research. I design novel pretext tasks and contrastive loss functions that challenge a model to learn the intrinsic structure of unlabeled data. Instead of relying on human-provided labels, the model learns by solving puzzles that force it to understand the underlying rules and symmetries of the data. This leads to powerful representations that excel in downstream tasks, even with limited supervision.

Computer Vision: This is a core application domain for my research. The visual world is governed by inherent structureâ€”objects transform, scenes are composed of parts, and perspectives change in predictable ways. I leverage my structure-driven approach to tackle fundamental challenges in computer vision. My specific interests include:

Developing robust models for object recognition, scene understanding, and semantic segmentation that require less labeled data.

Building representations that are naturally resilient to variations in viewpoint, lighting, and scale, leading to more reliable real-world systems.

Advancing generative vision models to create more realistic and controllable image synthesis by enforcing a deeper understanding of visual composition and semantics.

Large Language Models (LLMs) and Natural Language Processing (NLP): I am actively applying these same principles to the domain of language. Language possesses a deep and complex structure (grammatical, syntactical, and semantic), making it a fertile ground for my research. My interests in this area include:

Developing methods to improve the data efficiency and training stability of LLMs.

Enhancing the logical reasoning capabilities of language models by encouraging more structured internal representations of text.

Investigating how to ensure greater consistency and factual accuracy in the outputs of generative models.

Deep Learning Architecture and Theory: I work on developing deep learning models that are inherently more structured and less of a "black box." My research explores how mathematical principles can guide the design of neural network architectures, leading to systems that are more transparent, predictable, and verifiable. My background in pure mathematics provides me with the tools to analyze these models from a theoretical perspective, investigating their fundamental properties and performance guarantees.

Federated Learning: I am particularly interested in extending structured representation learning to decentralized environments. Federated learning poses unique challenges related to data privacy, security, and statistical heterogeneity. I explore how mathematically-grounded models can improve the efficiency, robustness, and fairness of learning across multiple distributed clients without centralizing sensitive data. My work aims to create models that are both privacy-preserving and resilient to the variations in data found in real-world decentralized systems.
